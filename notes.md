# run_1_distilBERT.ipynb

For this run I trained a small NN head on top of a distilBERT LLM with hidden size 768, layers 6, heads 12. The model is very simple: text is tokenized using distilbert-base-uncased with max length 512, passed to LLM where I take the last layer output (size 768) for A and B, concatenate them, then pass them into the head. This produces logits which are classified A/B/Tie. For batch size 32, 3 epochs with AdamW and lr of 1e-5 resulted in accuracy no better than chance (~33%). Afte this initial experiment, the next logic step is to inject information about the prompt into the training. The most challenging aspect was getting Intel's XPUs to play nice on a Windows 11 machine.

# run_2_appendprompt.ipynb

For this run I used the previous run 1 results but appended the tokenized prompt to the beginning of the responses. This increased the compute needs which required me to move to a batch size of 2 and single epoch to conserve memory on my machine and reduce the time of the experiment. The accuracy of the test set was ~45%, which was a considerable improvement over run 1. However, it is unclear if that is due to changing batch size or the prompt appending. Now that we have a direction, the next step is to improve the compute situation so more and larger experiments can be ran.

# Attempt to increase compute

Seeing as I deleted my AWS account and cannot access it, I turned to runpod.io for access to GPUs. I also switched to WSL for development on my Windows 11 machine, and plan to run tests locally and deploy training runs on the GPUs. Another difficulty is setting up a rational workflow between local and deployment environments. I am able to ssh into the runpod machines, but cannot easily connect to VSCodes workspace, so I must switch to production runs using .py files rather than .ipynb. Setting up a virtual environment on runpod was also a pain. I ended up putting together a setup.sh file to standardize everything. Thankfully it worked on CPU and GPU machines. The biggest problem was getting the ssh to work correctly. I think VSCode and WSL are having trouble playing nice because the config is autogenerated in a windows path i.e. user/.ssh and the id files are in a linux path ~/.ssh, so the config cannot find the id files. I fixed by adding the id files to the windows .ssh folder. More rigorus study of what exactly VSCode is using for ssh operations is needed. This ended up not mattering because I ended up using github and kaggle to transfer files. After two days I got CPU and GPU (1 A40) to work with ~$4 / month is storage. 

# Return to run_1_distilBERT.ipynb

Because I was using just the terminal to run experiments in the CPU/GPU machines on runpod I needed to convert the notebook to a file, introduce more sophisticated saving and QOL features (printing, logging, returning to old weights etc). AI was very helpful in quickly iterating and adding these features. I suspect that my experimental conditions have changed going from notebook to python file because accuracy shot up to 50%. My two hypothesis are (1) the max_length of the tokenizer changed and/or (2) the train/test/validation split may have changed. I will lower max_length to 128 and raise test and validation split to 10% each (note, this only reduced accuracy to 46%). The implementation was pretty trash to create a baseline so let's move onto run_2.

# Return to run_2_appendprompt.ipynb

This experiment just involved concatenating the prompt and response message and passing through the tokenization. This adds context at the expense of passing the max_length limit of the tokenizer/model combination. On my old local XPU this caused memory issues so I ran with batch size of 2 and only managed to get through a single epoch. Moving to the more powerful A40 I am able to using standard batch size 32 for 3 epochs without issue. Training finished in less than an hour with test accuracy of 47.3%. 

# Beyond the baseline

Appending the prompt to the response is one way of improving performance more along the lines of more efficiently using the given model's context. This will act as a good baseline. Right now I'm using distilbert-base-uncased from Huggingface. According to https://arxiv.org/abs/1910.01108 distilbert has 66M parameters. Let's run experiments along the following lines: (1) number of tuned parameters, (2) number of total parameters, (3) pre-training.

For experiment set 1 using distilbert-base-uncased:
- random: 35.1%
- baseline: 47.3%
- fully frozen: 45.3%
- bottom frozen: 46.9%

Clearly training on more parameters has a positive effect on accuracy. As a next step we are trying out models with greater raw capacity such as BERT and a large BERT. I also have started to organize the codebase now that some experiments are more steady (moving dataloader and model classes to core.py). Running these experiments took far more time than a 1XA40 could handle so I upgraded to 4XA40 which required some infrastructure changes and use of tmux to run sessions while I was out. For future reference I needed to make the following changes to the code for DDS (multi-GPU for torch):

1. Run with torchrun --nproc_per_node=4 python_script.py...
2. Get local ranks and set some communications parameters
        DEVICE = int(os.environ["LOCAL_RANK"])
        torch.cuda.set_device(DEVICE)
        dist.init_process_group(backend="gloo")
3. Modify BATCH_SIZE = max(1, args.batch // WORLD_SIZE)
4. Only save and log if RANK == 0 (main process)
5. if USE_DDP: dist.barrier() was needed to make sure RANK == 0 goes first
6. Wrap dataloaders in DistributedSampler and model in if model = DDP(model, device_ids=[DEVICE], find_unused_parameters=True). This turned out to be critical as my LLM implementation does not run some parameters during inference.
7. dist.broadcast(start_epoch_tensor, src=0) and train_sampler.set_epoch(epoch) needed during training.
8. dist.all_reduce(val_loss_tensor, op=dist.ReduceOp.SUM) needed for non-training calculations

For experiment set 2, using bert models to study size, we found some improvement by going with larger models:
- distilbert: 47.3%
- bert: 48.35%
- large bert: 48.43%

For experiment set 3 we will use similar models with different training:

roberta-base: loss function is 15% MLM masking -> 80% of cases replaced with <mask>, 10% replaced by random token, 10% are left, case sensitive english, training data is BookCorpus, English Wikipedia, CC-News, OpenWebText, Stories ~160GB, tokenized using BPE, new docs marked with <s>. 48.6%

google/electra-base-discriminator: a generator LLM creates errors in data, discriminator is binary classified to detect real/fake tokens. 49.3%

microsoft/deberta-base: 35.1%

albert-base-v2: 49.1%

# Optimizations

Some ideas for some further optimizations:

1. Pytorch Lightining
2. Hyperparameter tuning
3. Changing data/classifier architecture

Pyotrch Lightining
# Single GPU/CPU
python run_6_light.py --epochs 3 --batch 32

# Multiple GPUs with DDP
python run_6_light.py --epochs 3 --batch 32 --devices 2 --strategy ddp

# CPU only
python run_6_light.py --epochs 3 --batch 32

# Resume from checkpoint
python run_6_light.py --resume_timestamp 20231215_120000

# Smoke test
python run_6_light.py --smoke