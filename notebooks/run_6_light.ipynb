{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Response Classifier - PyTorch Lightning Training\n",
    "\n",
    "This notebook trains a transformer-based model to classify LLM response comparisons using PyTorch Lightning.\n",
    "\n",
    "## Features\n",
    "- PyTorch Lightning for simplified training\n",
    "- Multiple transformer model support (BERT, DistilBERT, RoBERTa, ELECTRA, DeBERTa, ALBERT)\n",
    "- Automatic GPU/CPU detection\n",
    "- Multi-GPU training support\n",
    "- Checkpoint saving and resuming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer,\n",
    "                          RobertaModel, RobertaTokenizer, ElectraModel, ElectraTokenizerFast,\n",
    "                          DebertaModel, DebertaTokenizer, AlbertModel, AlbertTokenizer, AutoTokenizer)\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# Add parent directory to path to import local modules\n",
    "sys.path.append('..')\n",
    "from core import ResponseDataset, ResponseScorer\n",
    "\n",
    "# Set matmul precision for better performance on modern GPUs\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_REGISTRY = {\n",
    "    \"bert-base-uncased\": (BertModel, BertTokenizer),\n",
    "    \"distilbert-base-uncased\": (DistilBertModel, DistilBertTokenizer),\n",
    "    \"roberta-base\": (RobertaModel, RobertaTokenizer),\n",
    "    \"google/electra-base-discriminator\": (ElectraModel, ElectraTokenizerFast),\n",
    "    \"microsoft/deberta-base\": (DebertaModel, DebertaTokenizer),\n",
    "    \"albert-base-v2\": (AlbertModel, AlbertTokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningResponseScorer(pl.LightningModule):\n",
    "    def __init__(self, model_class, base_model_name, weights_dir, lr=1e-5, smoke_test=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model_class'])\n",
    "        self.lr = lr\n",
    "        self.smoke_test = smoke_test\n",
    "\n",
    "        # Create a simple logging function for model initialization\n",
    "        def log_fn(msg, rank_specific=False):\n",
    "            print(msg)\n",
    "\n",
    "        # Initialize the ResponseScorer model\n",
    "        self.model = ResponseScorer.from_pretrained(\n",
    "            model_class=model_class,\n",
    "            base_model_name=base_model_name,\n",
    "            weights_dir=weights_dir,\n",
    "            smoke_test=smoke_test,\n",
    "            rank=self.global_rank if hasattr(self, 'global_rank') else 0,\n",
    "            log_fn=log_fn\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, ids_a, mask_a, ids_b, mask_b):\n",
    "        return self.model(ids_a, mask_a, ids_b, mask_b)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ids_a = batch[\"input_ids_a\"]\n",
    "        mask_a = batch[\"attention_mask_a\"]\n",
    "        ids_b = batch[\"input_ids_b\"]\n",
    "        mask_b = batch[\"attention_mask_b\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        logits = self(ids_a, mask_a, ids_b, mask_b)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ids_a = batch[\"input_ids_a\"]\n",
    "        mask_a = batch[\"attention_mask_a\"]\n",
    "        ids_b = batch[\"input_ids_b\"]\n",
    "        mask_b = batch[\"attention_mask_b\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        logits = self(ids_a, mask_a, ids_b, mask_b)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        ids_a = batch[\"input_ids_a\"]\n",
    "        mask_a = batch[\"attention_mask_a\"]\n",
    "        ids_b = batch[\"input_ids_b\"]\n",
    "        mask_b = batch[\"attention_mask_b\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        logits = self(ids_a, mask_a, ids_b, mask_b)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "\n",
    "        self.log('test_loss', loss, on_epoch=True, sync_dist=True)\n",
    "        self.log('test_acc', acc, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "SMOKE_TEST = False  # Set to True for quick testing\n",
    "BASE_MODEL = 'distilbert-base-uncased'  # Choose from MODEL_REGISTRY keys\n",
    "TAG = ''  # Optional tag for organizing runs\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-5\n",
    "MAX_LEN = 512\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = '../data/train.csv'\n",
    "WEIGHTS_DIR = os.path.join('../weights', BASE_MODEL)\n",
    "ROOT_CKPT = '../checkpoints'\n",
    "\n",
    "# For resuming training\n",
    "RESUME_TIMESTAMP = None  # Set to timestamp string to resume (e.g., '20231215_120000')\n",
    "\n",
    "# Override for smoke test\n",
    "if SMOKE_TEST:\n",
    "    NUM_EPOCHS = 1\n",
    "    BATCH_SIZE = 1\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Base Model: {BASE_MODEL}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {LR}\")\n",
    "print(f\"  Smoke Test: {SMOKE_TEST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Checkpoint Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BASE_MODEL not in MODEL_REGISTRY:\n",
    "    raise ValueError(f\"Unsupported base_model '{BASE_MODEL}'. Available: {list(MODEL_REGISTRY.keys())}\")\n",
    "\n",
    "# Setup checkpoint directory\n",
    "if RESUME_TIMESTAMP is None:\n",
    "    TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    CKPT_DIR = os.path.join(ROOT_CKPT, TIMESTAMP, TAG)\n",
    "else:\n",
    "    TIMESTAMP = RESUME_TIMESTAMP\n",
    "    CKPT_DIR = os.path.join(ROOT_CKPT, RESUME_TIMESTAMP, TAG)\n",
    "    if not os.path.isdir(os.path.join(ROOT_CKPT, RESUME_TIMESTAMP)):\n",
    "        raise ValueError(f\"Checkpoint directory {os.path.join(ROOT_CKPT, RESUME_TIMESTAMP)} does not exist.\")\n",
    "\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoint directory: {CKPT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "\n",
    "# Load tokenizer\n",
    "model_class, _ = MODEL_REGISTRY[BASE_MODEL]\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Load and split dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "\n",
    "df_train, df_temp = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ResponseDataset(df_train, tokenizer, MAX_LEN)\n",
    "val_dataset = ResponseDataset(df_val, tokenizer, MAX_LEN)\n",
    "test_dataset = ResponseDataset(df_test, tokenizer, MAX_LEN)\n",
    "\n",
    "print(\"Dataset ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of workers - use 0 for CPU to avoid multiprocessing issues\n",
    "num_workers = 0 if not torch.cuda.is_available() else NUM_WORKERS\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"DataLoaders created with {num_workers} workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing model...\")\n",
    "\n",
    "model = LightningResponseScorer(\n",
    "    model_class=model_class,\n",
    "    base_model_name=BASE_MODEL,\n",
    "    weights_dir=WEIGHTS_DIR,\n",
    "    lr=LR,\n",
    "    smoke_test=SMOKE_TEST\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"ResponseScorer has {total_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Setup Callbacks and Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=CKPT_DIR,\n",
    "    filename='epoch{epoch:02d}',\n",
    "    save_top_k=-1,  # Save all checkpoints\n",
    "    every_n_epochs=1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Setup logger\n",
    "csv_logger = CSVLogger(save_dir=CKPT_DIR, name='lightning_logs')\n",
    "\n",
    "print(\"Callbacks and logger configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Determine Checkpoint for Resuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine checkpoint path for resuming\n",
    "ckpt_path = None\n",
    "if RESUME_TIMESTAMP is not None:\n",
    "    ckpt_files = [f for f in os.listdir(CKPT_DIR) if f.startswith(\"epoch\") and f.endswith(\".ckpt\")]\n",
    "    if ckpt_files:\n",
    "        ckpt_files.sort(key=lambda x: int(x.replace(\"epoch\", \"\").replace(\".ckpt\", \"\")))\n",
    "        latest_ckpt = ckpt_files[-1]\n",
    "        ckpt_path = os.path.join(CKPT_DIR, latest_ckpt)\n",
    "        print(f\"Resuming from {latest_ckpt}\")\n",
    "    else:\n",
    "        print(\"Resume timestamp given, but no checkpoints found. Starting from scratch.\")\n",
    "else:\n",
    "    print(\"Starting fresh training run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine accelerator and devices\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = \"gpu\"\n",
    "    devices = torch.cuda.device_count()  # Use all available GPUs\n",
    "    print(f\"Using GPU acceleration with {devices} device(s)\")\n",
    "else:\n",
    "    accelerator = \"cpu\"\n",
    "    devices = 1  # CPU always uses 1 device in Lightning\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Adjust strategy for single device\n",
    "strategy = \"auto\" if devices == 1 else \"ddp\"  # Use DDP for multi-GPU\n",
    "\n",
    "# Setup trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    devices=devices,\n",
    "    accelerator=accelerator,\n",
    "    strategy=strategy,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=csv_logger,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    "    deterministic=False,\n",
    "    fast_dev_run=SMOKE_TEST\n",
    ")\n",
    "\n",
    "print(\"Trainer configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.fit(model, train_loader, val_loader, ckpt_path=ckpt_path)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing...\")\n",
    "test_results = trainer.test(model, test_loader)\n",
    "print(\"Testing complete!\")\n",
    "print(f\"\\nTest Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. View Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display training metrics\n",
    "import glob\n",
    "\n",
    "metrics_file = glob.glob(os.path.join(CKPT_DIR, 'lightning_logs', '**', 'metrics.csv'), recursive=True)\n",
    "if metrics_file:\n",
    "    metrics_df = pd.read_csv(metrics_file[0])\n",
    "    print(\"\\nTraining Metrics:\")\n",
    "    display(metrics_df)\n",
    "else:\n",
    "    print(\"No metrics file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Plot Training History (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if metrics_file:\n",
    "    # Plot loss\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(metrics_df['epoch'].dropna(), metrics_df['train_loss_epoch'].dropna(), label='Train Loss', marker='o')\n",
    "    axes[0].plot(metrics_df['epoch'].dropna(), metrics_df['val_loss'].dropna(), label='Val Loss', marker='s')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(metrics_df['epoch'].dropna(), metrics_df['train_acc_epoch'].dropna(), label='Train Acc', marker='o')\n",
    "    axes[1].plot(metrics_df['epoch'].dropna(), metrics_df['val_acc'].dropna(), label='Val Acc', marker='s')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CKPT_DIR, 'training_history.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Plot saved to {os.path.join(CKPT_DIR, 'training_history.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides an interactive way to train the LLM response classifier using PyTorch Lightning. You can:\n",
    "\n",
    "- Adjust hyperparameters in cell 4\n",
    "- Run cells individually to inspect intermediate results\n",
    "- Visualize training progress\n",
    "- Resume from checkpoints\n",
    "- Use multiple GPUs automatically\n",
    "\n",
    "All checkpoints and logs are saved to the checkpoint directory for later analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 16. Generate Submission File from Test Data\n\nThis cell loads the test.csv file, runs inference with the trained model, and creates a submission.csv file with probability predictions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\n\n# Create a simple dataset for test data (no labels)\nclass TestDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        t_a = self.tokenizer(row[\"prompt\"] + row[\"response_a\"], \n                             max_length=self.max_len,\n                             padding=\"max_length\", \n                             truncation=True, \n                             return_tensors=\"pt\")\n        t_b = self.tokenizer(row[\"prompt\"] + row[\"response_b\"], \n                             max_length=self.max_len,\n                             padding=\"max_length\", \n                             truncation=True, \n                             return_tensors=\"pt\")\n        return {\n            \"id\": row[\"id\"],\n            \"input_ids_a\": t_a[\"input_ids\"].squeeze(0),\n            \"attention_mask_a\": t_a[\"attention_mask\"].squeeze(0),\n            \"input_ids_b\": t_b[\"input_ids\"].squeeze(0),\n            \"attention_mask_b\": t_b[\"attention_mask\"].squeeze(0)\n        }\n\nprint(\"Loading test data...\")\ntest_df = pd.read_csv('../data/test.csv')\nprint(f\"Test samples: {len(test_df)}\")\n\n# Create test dataset and dataloader\ntest_dataset_submission = TestDataset(test_df, tokenizer, MAX_LEN)\ntest_loader_submission = DataLoader(\n    test_dataset_submission,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=torch.cuda.is_available()\n)\n\n# Set model to evaluation mode\nmodel.eval()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Run inference\npredictions = []\nids = []\n\nprint(\"Running inference on test data...\")\nwith torch.no_grad():\n    for batch in tqdm(test_loader_submission, desc=\"Generating predictions\"):\n        batch_ids = batch[\"id\"]\n        ids_a = batch[\"input_ids_a\"].to(device)\n        mask_a = batch[\"attention_mask_a\"].to(device)\n        ids_b = batch[\"input_ids_b\"].to(device)\n        mask_b = batch[\"attention_mask_b\"].to(device)\n        \n        # Get logits from model\n        logits = model(ids_a, mask_a, ids_b, mask_b)\n        \n        # Convert logits to probabilities using softmax\n        probs = F.softmax(logits, dim=-1)\n        \n        # Store predictions\n        for i, batch_id in enumerate(batch_ids):\n            ids.append(batch_id)\n            # probs shape: (batch_size, 3) where classes are [model_b_wins, tie, model_a_wins]\n            # Index 0 = model_b wins, Index 1 = tie, Index 2 = model_a wins\n            predictions.append({\n                'id': batch_id,\n                'winner_model_a': probs[i, 2].cpu().item(),  # Probability that model_a wins\n                'winner_model_b': probs[i, 0].cpu().item(),  # Probability that model_b wins\n                'winner_tie': probs[i, 1].cpu().item()       # Probability of tie\n            })\n\n# Create submission dataframe\nsubmission_df = pd.DataFrame(predictions)\n\n# Save to CSV\nsubmission_path = '../submission.csv'\nsubmission_df.to_csv(submission_path, index=False)\n\nprint(f\"\\nSubmission file saved to: {submission_path}\")\nprint(f\"Shape: {submission_df.shape}\")\nprint(f\"\\nFirst few rows:\")\nprint(submission_df.head())\n\n# Verify probabilities sum to 1\nprint(f\"\\nVerifying probabilities sum to 1.0:\")\nprob_sums = submission_df[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1)\nprint(f\"Min sum: {prob_sums.min():.6f}\")\nprint(f\"Max sum: {prob_sums.max():.6f}\")\nprint(f\"Mean sum: {prob_sums.mean():.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}