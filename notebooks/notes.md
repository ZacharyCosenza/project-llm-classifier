# run_1_distilBERT.ipynb

For this run I trained a small NN head on top of a distilBERT LLM with hidden size 768, layers 6, heads 12. The model is very simple: text is tokenized using distilbert-base-uncased with max length 512, passed to LLM where I take the last layer output (size 768) for A and B, concatenate them, then pass them into the head. This produces logits which are classified A/B/Tie. For batch size 32, 3 epochs with AdamW and lr of 1e-5 resulted in accuracy no better than chance (~33%). Afte this initial experiment, the next logic step is to inject information about the prompt into the training. The most challenging aspect was getting Intel's XPUs to play nice on a Windows 11 machine.

# run_2_appendprompt.ipynb

For this run I used the previous run 1 results but appended the tokenized prompt to the beginning of the responses. This increased the compute needs which required me to move to a batch size of 2 and single epoch to conserve memory on my machine and reduce the time of the experiment. The accuracy of the test set was ~45%, which was a considerable improvement over run 1. However, it is unclear if that is due to changing batch size or the prompt appending. Now that we have a direction, the next step is to improve the compute situation so more and larger experiments can be ran.

# Attempt to increase compute

Seeing as I deleted my AWS account and cannot access it, I turned to runpod.io for access to GPUs. I also switched to WSL for development on my Windows 11 machine, and plan to run tests locally and deploy training runs on the GPUs. Another difficulty is setting up a rational workflow between local and deployment environments. I am able to ssh into the runpod machines, but cannot easily connect to VSCodes workspace, so I must switch to production runs using .py files rather than .ipynb. Setting up a virtual environment on runpod was also a pain. I ended up putting together a setup.sh file to standardize everything. Thankfully it worked on CPU and GPU machines. The biggest problem was getting the ssh to work correctly. I think VSCode and WSL are having trouble playing nice because the config is autogenerated in a windows path i.e. user/.ssh and the id files are in a linux path ~/.ssh, so the config cannot find the id files. I fixed by adding the id files to the windows .ssh folder. More rigorus study of what exactly VSCode is using for ssh operations is needed. This ended up not mattering because I ended up using github and kaggle to transfer files. After two days I got CPU and GPU (1 A40) to work with ~$4 / month is storage. 

# Return to run_1_distilBERT.ipynb

Because I was using just the terminal to run experiments in the CPU/GPU machines on runpod I needed to convert the notebook to a file, introduce more sophisticated saving and QOL features (printing, logging, returning to old weights etc). AI was very helpful in quickly iterating and adding these features. I suspect that my experimental conditions have changed going from notebook to python file because accuracy shot up to 50%. My two hypothesis are (1) the max_length of the tokenizer changed and/or (2) the train/test/validation split may have changed. I will lower max_length to 128 and raise test and validation split to 10% each (note, this only reduced accuracy to 46%). The implementation was pretty trash to create a baseline so let's move onto run_2.

# Return to run_2_appendprompt.ipynb

This experiment just involved concatenating the prompt and response message and passing through the tokenization. This adds context at the expense of passing the max_length limit of the tokenizer/model combination. On my old local XPU this caused memory issues so I ran with batch size of 2 and only managed to get through a single epoch. Moving to the more powerful A40 I am able to using standard batch size 32 for 3 epochs without issue. Training finished in less than an hour with test accuracy of 47.3%. 

# Let's scale the base LLM

Appending the prompt to the response is one way of improving performance more along the lines of more efficiently using the given model's context. Let's scale the model itself. Right now I'm using distilbert-base-uncased from Huggingface. According to https://arxiv.org/abs/1910.01108 distilbert has 66M parameters. 