{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916e0658",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c6e0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>[\"How can I create a test set for a very rare ...</td>\n",
       "      <td>[\"Creating a test set for a very rare category...</td>\n",
       "      <td>[\"When building a classifier for a very rare c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n",
       "      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n",
       "      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "2  [\"explain function calling. how would you call...   \n",
       "3  [\"How can I create a test set for a very rare ...   \n",
       "4  [\"What is the best way to travel from Tel-Aviv...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "2  [\"Function calling is the process of invoking ...   \n",
       "3  [\"Creating a test set for a very rare category...   \n",
       "4  [\"The best way to travel from Tel Aviv to Jeru...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "2  [\"Function calling is the process of invoking ...               0   \n",
       "3  [\"When building a classifier for a very rare c...               1   \n",
       "4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  \n",
       "2               0           1  \n",
       "3               0           0  \n",
       "4               1           0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = 'data/train.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c345cb",
   "metadata": {},
   "source": [
    "### Download/Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a75c09c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaccosenza/code/project-llm-classifier/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded saved DistilBERT weights.\n",
      "Model type: DistilBertModel\n",
      "Tokenizer vocab size: 30522\n",
      "Hidden size: 768, Layers: 6, Heads: 12\n",
      "Inference QC: last_hidden_state shape torch.Size([1, 9, 768])\n",
      "Training QC: backward pass successful (dummy loss=-0.0098)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
    "import os\n",
    "\n",
    "weights = 'pretrained'\n",
    "model_dir = \"weights/distilbert\"\n",
    "\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Option A: Download and save pretrained weights\n",
    "def download_and_save():\n",
    "    model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model.save_pretrained(model_dir)\n",
    "    print(\"Downloaded and saved pretrained DistilBERT weights.\")\n",
    "    return model\n",
    "\n",
    "# Option B: Load saved weights (if available)\n",
    "def load_saved():\n",
    "    model = DistilBertModel.from_pretrained(model_dir)\n",
    "    print(\"Loaded saved DistilBERT weights.\")\n",
    "    return model\n",
    "\n",
    "# Option C: Randomly initialize weights\n",
    "def init_random():\n",
    "    config = DistilBertConfig()\n",
    "    model = DistilBertModel(config)\n",
    "    print(\"Initialized DistilBERT with random weights.\")\n",
    "    return model\n",
    "\n",
    "# Load weights\n",
    "if os.path.exists(os.path.join(model_dir, \"model.safetensors\")):\n",
    "    base_model = load_saved()\n",
    "else:\n",
    "    base_model = download_and_save() if weights == 'pretrained' else init_random()\n",
    "\n",
    "# --- Model and Tokenizer ---\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(f\"Model type: {type(base_model).__name__}\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Hidden size: {base_model.config.dim}, Layers: {base_model.config.n_layers}, Heads: {base_model.config.n_heads}\")\n",
    "\n",
    "# --- Inference QC ---\n",
    "sample_text = \"I love using Hugging Face Transformers!\"\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\").to(device)\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = base_model(**inputs)\n",
    "\n",
    "print(f\"Inference QC: last_hidden_state shape {outputs.last_hidden_state.shape}\")\n",
    "\n",
    "# --- Training QC ---\n",
    "base_model.train()\n",
    "inputs = tokenizer(list(df[\"prompt\"][:2]), return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "optimizer = torch.optim.AdamW(base_model.parameters(), lr=1e-5)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "outputs = base_model(**inputs)\n",
    "loss = outputs.last_hidden_state.mean()  # dummy scalar loss just for gradient test\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(f\"Training QC: backward pass successful (dummy loss={loss.item():.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afef08f",
   "metadata": {},
   "source": [
    "### Dataset/Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79b60b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ResponseDataset(Dataset):\n",
    "    def __init__(self, df, max_length=512):\n",
    "        self.df = df\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def outcome_to_class(self, row):\n",
    "        if row[\"winner_model_a\"] == 1:\n",
    "            return 2   # A wins\n",
    "        if row[\"winner_model_b\"] == 1:\n",
    "            return 0   # B wins\n",
    "        return 1       # tie\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        tokens_a = tokenizer(\n",
    "            row[\"response_a\"], max_length=self.max_length,\n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        tokens_b = tokenizer(\n",
    "            row[\"response_b\"], max_length=self.max_length,\n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        label = torch.tensor(self.outcome_to_class(row), dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids_a\": tokens_a[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_a\": tokens_a[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_b\": tokens_b[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_b\": tokens_b[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": label,\n",
    "        }\n",
    "\n",
    "df_train, df_temp = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ResponseDataset(df_train)\n",
    "val_dataset = ResponseDataset(df_val)\n",
    "test_dataset = ResponseDataset(df_test)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 2\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573564ad",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d962bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ResponseScorer(nn.Module):\n",
    "    def __init__(self, base_model, freeze_base=False, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        hidden_dim = self.base_model.config.dim\n",
    "\n",
    "        # classifier head → outputs logits for [B wins, Tie, A wins]\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, 3)   # 3-class output\n",
    "        )\n",
    "\n",
    "        if freeze_base:\n",
    "            for p in self.base_model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        out = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return out.last_hidden_state[:, 0, :]  # CLS\n",
    "\n",
    "    def forward(self, input_ids_a, mask_a, input_ids_b, mask_b):\n",
    "        # embeddings from base model\n",
    "        h_a = self.encode(input_ids_a.to(self.device), mask_a.to(self.device))\n",
    "        h_b = self.encode(input_ids_b.to(self.device), mask_b.to(self.device))\n",
    "\n",
    "        # concatenate the two embeddings\n",
    "        h = torch.cat([h_a, h_b], dim=-1)\n",
    "\n",
    "        # 3-class logits\n",
    "        logits = self.head(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c9ab04",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a79a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0.0% done | Avg Loss: 1.0923"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m logits = model(input_ids_a, mask_a, input_ids_b, mask_b)\n\u001b[32m     36\u001b[39m loss = loss_fn(logits, labels)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m optimizer.step()\n\u001b[32m     40\u001b[39m running_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/project-llm-classifier/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/project-llm-classifier/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/project-llm-classifier/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "mode = 'train'\n",
    "\n",
    "model = ResponseScorer(base_model, freeze_base=False, device=device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "if mode == 'train':\n",
    "\n",
    "    num_epochs = 3\n",
    "    val_losses, val_accuracies = [], []\n",
    "\n",
    "    # Create directory for checkpoints\n",
    "    if not os.path.isdir(\"checkpoints\"):\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    save_every = 100  # batches\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(train_loader)\n",
    "\n",
    "        for i, batch in enumerate(train_loader, 1):\n",
    "            input_ids_a = batch[\"input_ids_a\"].to(device)\n",
    "            mask_a = batch[\"attention_mask_a\"].to(device)\n",
    "            input_ids_b = batch[\"input_ids_b\"].to(device)\n",
    "            mask_b = batch[\"attention_mask_b\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)        # <-- single class ID 0/1/2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # logits: [batch, 3]\n",
    "            logits = model(input_ids_a, mask_a, input_ids_b, mask_b)\n",
    "\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # ---- progress indicator ----\n",
    "            pct_done = 100 * i / total_batches\n",
    "            print(\n",
    "                f\"\\rEpoch {epoch}: {pct_done:.1f}% done | Avg Loss: {running_loss / i:.4f}\",\n",
    "                end=\"\"\n",
    "            )\n",
    "\n",
    "            # ---- periodic save ----\n",
    "            if i % save_every == 0:\n",
    "                ckpt_path = f\"checkpoints/epoch{epoch}_batch{i}.pt\"\n",
    "                torch.save(model.state_dict(), ckpt_path)\n",
    "                print(f\"\\nSaved checkpoint to {ckpt_path}\")\n",
    "\n",
    "        print()  # newline after epoch progress\n",
    "\n",
    "        # Save final epoch weights\n",
    "        torch.save(model.state_dict(), f\"checkpoints/epoch{epoch}_final.pt\")\n",
    "\n",
    "        # ----------------------\n",
    "        # Validation\n",
    "        # ----------------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids_a = batch[\"input_ids_a\"].to(device)\n",
    "                mask_a = batch[\"attention_mask_a\"].to(device)\n",
    "                input_ids_b = batch[\"input_ids_b\"].to(device)\n",
    "                mask_b = batch[\"attention_mask_b\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                logits = model(input_ids_a, mask_a, input_ids_b, mask_b)\n",
    "\n",
    "                val_loss += loss_fn(logits, labels).item()\n",
    "\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = correct / total\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch} done — Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15be954",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60d251",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m mask_b = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask_b\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m     24\u001b[39m labels = batch[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m preds = \u001b[43mlogits\u001b[49m.argmax(dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     27\u001b[39m correct += (preds == labels).sum().item()\n\u001b[32m     28\u001b[39m total += labels.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "path_pretrain = None\n",
    "\n",
    "# Instantiate model + scorer\n",
    "if path_pretrain is not None:\n",
    "    model = ResponseScorer(base_model, freeze_base=False, device=device)\n",
    "    model.load_state_dict(torch.load(path_pretrain, map_location=device))\n",
    "    model.to(device)\n",
    "else:\n",
    "    model = ResponseScorer(base_model, freeze_base=False, device=device)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "num_batches = len(test_loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader, 1):\n",
    "        input_ids_a = batch[\"input_ids_a\"].to(device)\n",
    "        mask_a = batch[\"attention_mask_a\"].to(device)\n",
    "        input_ids_b = batch[\"input_ids_b\"].to(device)\n",
    "        mask_b = batch[\"attention_mask_b\"].to(device)\n",
    "\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        logits = model(input_ids_a, mask_a, input_ids_b, mask_b)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        pct = 100 * i / num_batches\n",
    "        print(f\"[{pct:6.2f}%] Running Test Accuracy: {correct / total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02865bb6",
   "metadata": {},
   "source": [
    "### Kaggle Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd42a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_to_datset(row, max_length = 512):\n",
    "\n",
    "    tokens_a = tokenizer(\n",
    "                row[\"response_a\"], max_length=max_length,\n",
    "                    padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "                )\n",
    "    tokens_b = tokenizer(\n",
    "        row[\"response_b\"], max_length=max_length,\n",
    "        padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return [tokens_a[\"input_ids\"].reshape(1,-1), \n",
    "            tokens_a[\"attention_mask\"].reshape(1,-1), \n",
    "            tokens_b[\"input_ids\"].reshape(1,-1), \n",
    "            tokens_b[\"attention_mask\"].reshape(1,-1),]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(df_test.shape[0]):\n",
    "        test_set = kaggle_to_datset(df_test.iloc[i])\n",
    "        logits = model(*test_set)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        print(f'prediction is {preds} for test set {i}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
