{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916e0658",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c6e0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>[\"How can I create a test set for a very rare ...</td>\n",
       "      <td>[\"Creating a test set for a very rare category...</td>\n",
       "      <td>[\"When building a classifier for a very rare c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n",
       "      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n",
       "      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "2  [\"explain function calling. how would you call...   \n",
       "3  [\"How can I create a test set for a very rare ...   \n",
       "4  [\"What is the best way to travel from Tel-Aviv...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "2  [\"Function calling is the process of invoking ...   \n",
       "3  [\"Creating a test set for a very rare category...   \n",
       "4  [\"The best way to travel from Tel Aviv to Jeru...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "2  [\"Function calling is the process of invoking ...               0   \n",
       "3  [\"When building a classifier for a very rare c...               1   \n",
       "4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  \n",
       "2               0           1  \n",
       "3               0           0  \n",
       "4               1           0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = 'data/train.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c345cb",
   "metadata": {},
   "source": [
    "### Download/Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a75c09c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacco\\OneDrive\\Documents\\Personal\\Code\\venvs\\xpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: xpu\n",
      "Loaded saved DistilBERT weights.\n",
      "Model type: DistilBertModel\n",
      "Tokenizer vocab size: 30522\n",
      "Hidden size: 768, Layers: 6, Heads: 12\n",
      "Inference QC: last_hidden_state shape torch.Size([1, 9, 768])\n",
      "Training QC: backward pass successful (dummy loss=-0.0097)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
    "import os\n",
    "\n",
    "weights = 'pretrained'\n",
    "model_dir = \"weights/distilbert\"\n",
    "\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Option A: Download and save pretrained weights\n",
    "def download_and_save():\n",
    "    model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model.save_pretrained(model_dir)\n",
    "    print(\"Downloaded and saved pretrained DistilBERT weights.\")\n",
    "    return model\n",
    "\n",
    "# Option B: Load saved weights (if available)\n",
    "def load_saved():\n",
    "    model = DistilBertModel.from_pretrained(model_dir)\n",
    "    print(\"Loaded saved DistilBERT weights.\")\n",
    "    return model\n",
    "\n",
    "# Option C: Randomly initialize weights\n",
    "def init_random():\n",
    "    config = DistilBertConfig()\n",
    "    model = DistilBertModel(config)\n",
    "    print(\"Initialized DistilBERT with random weights.\")\n",
    "    return model\n",
    "\n",
    "# Load weights\n",
    "if os.path.exists(os.path.join(model_dir, \"model.safetensors\")):\n",
    "    base_model = load_saved()\n",
    "else:\n",
    "    base_model = download_and_save() if weights == 'pretrained' else init_random()\n",
    "\n",
    "# --- Model and Tokenizer ---\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(f\"Model type: {type(base_model).__name__}\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Hidden size: {base_model.config.dim}, Layers: {base_model.config.n_layers}, Heads: {base_model.config.n_heads}\")\n",
    "\n",
    "# --- Inference QC ---\n",
    "sample_text = \"I love using Hugging Face Transformers!\"\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\").to(device)\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = base_model(**inputs)\n",
    "\n",
    "print(f\"Inference QC: last_hidden_state shape {outputs.last_hidden_state.shape}\")\n",
    "\n",
    "# --- Training QC ---\n",
    "base_model.train()\n",
    "inputs = tokenizer(list(df[\"prompt\"][:2]), return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "optimizer = torch.optim.AdamW(base_model.parameters(), lr=1e-5)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "outputs = base_model(**inputs)\n",
    "loss = outputs.last_hidden_state.mean()  # dummy scalar loss just for gradient test\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(f\"Training QC: backward pass successful (dummy loss={loss.item():.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afef08f",
   "metadata": {},
   "source": [
    "### Dataset/Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "79b60b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ResponseDataset(Dataset):\n",
    "    def __init__(self, df, max_length=512):\n",
    "        self.df = df\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def outcome_to_class(self, row):\n",
    "        if row[\"winner_model_a\"] == 1:\n",
    "            return 2   # A wins\n",
    "        if row[\"winner_model_b\"] == 1:\n",
    "            return 0   # B wins\n",
    "        return 1       # tie\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        tokens_a = tokenizer(\n",
    "            row[\"response_a\"], max_length=self.max_length,\n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        tokens_b = tokenizer(\n",
    "            row[\"response_b\"], max_length=self.max_length,\n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        label = torch.tensor(self.outcome_to_class(row), dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids_a\": tokens_a[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_a\": tokens_a[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_b\": tokens_b[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_b\": tokens_b[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": label,\n",
    "        }\n",
    "\n",
    "df_train, df_temp = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ResponseDataset(df_train)\n",
    "val_dataset = ResponseDataset(df_val)\n",
    "test_dataset = ResponseDataset(df_test)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573564ad",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d962bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ResponseScorer(nn.Module):\n",
    "    def __init__(self, base_model, freeze_base=False, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        hidden_dim = self.base_model.config.dim\n",
    "\n",
    "        # classifier head → outputs logits for [B wins, Tie, A wins]\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, 3)   # 3-class output\n",
    "        )\n",
    "\n",
    "        if freeze_base:\n",
    "            for p in self.base_model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        out = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return out.last_hidden_state[:, 0, :]  # CLS\n",
    "\n",
    "    def forward(self, input_ids_a, mask_a, input_ids_b, mask_b):\n",
    "        # embeddings from base model\n",
    "        h_a = self.encode(input_ids_a.to(self.device), mask_a.to(self.device))\n",
    "        h_b = self.encode(input_ids_b.to(self.device), mask_b.to(self.device))\n",
    "\n",
    "        # concatenate the two embeddings\n",
    "        h = torch.cat([h_a, h_b], dim=-1)\n",
    "\n",
    "        # 3-class logits\n",
    "        logits = self.head(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c9ab04",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29a79a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "mode = 'test' # 'train'\n",
    "\n",
    "model = ResponseScorer(base_model, freeze_base=False, device=device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "if mode == 'train':\n",
    "\n",
    "    num_epochs = 3\n",
    "    val_losses, val_accuracies = [], []\n",
    "\n",
    "    # Create directory for checkpoints\n",
    "    if not os.path.isdir(\"checkpoints\"):\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    save_every = 100  # batches\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(train_loader)\n",
    "\n",
    "        for i, batch in enumerate(train_loader, 1):\n",
    "            input_ids_a = batch[\"input_ids_a\"].to(device)\n",
    "            mask_a = batch[\"attention_mask_a\"].to(device)\n",
    "            input_ids_b = batch[\"input_ids_b\"].to(device)\n",
    "            mask_b = batch[\"attention_mask_b\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)        # <-- single class ID 0/1/2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # logits: [batch, 3]\n",
    "            logits = model(input_ids_a, mask_a, input_ids_b, mask_b)\n",
    "\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # ---- progress indicator ----\n",
    "            pct_done = 100 * i / total_batches\n",
    "            print(\n",
    "                f\"\\rEpoch {epoch}: {pct_done:.1f}% done | Avg Loss: {running_loss / i:.4f}\",\n",
    "                end=\"\"\n",
    "            )\n",
    "\n",
    "            # ---- periodic save ----\n",
    "            if i % save_every == 0:\n",
    "                ckpt_path = f\"checkpoints/epoch{epoch}_batch{i}.pt\"\n",
    "                torch.save(model.state_dict(), ckpt_path)\n",
    "                print(f\"\\nSaved checkpoint to {ckpt_path}\")\n",
    "\n",
    "        print()  # newline after epoch progress\n",
    "\n",
    "        # Save final epoch weights\n",
    "        torch.save(model.state_dict(), f\"checkpoints/epoch{epoch}_final.pt\")\n",
    "\n",
    "        # ----------------------\n",
    "        # Validation\n",
    "        # ----------------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids_a = batch[\"input_ids_a\"].to(device)\n",
    "                mask_a = batch[\"attention_mask_a\"].to(device)\n",
    "                input_ids_b = batch[\"input_ids_b\"].to(device)\n",
    "                mask_b = batch[\"attention_mask_b\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                logits = model(input_ids_a, mask_a, input_ids_b, mask_b)\n",
    "\n",
    "                val_loss += loss_fn(logits, labels).item()\n",
    "\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = correct / total\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch} done — Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15be954",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ee60d251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.11%] Running Test Accuracy: 0.5000\n",
      "[  2.22%] Running Test Accuracy: 0.3906\n",
      "[  3.33%] Running Test Accuracy: 0.3333\n",
      "[  4.44%] Running Test Accuracy: 0.3125\n",
      "[  5.56%] Running Test Accuracy: 0.3125\n",
      "[  6.67%] Running Test Accuracy: 0.3073\n",
      "[  7.78%] Running Test Accuracy: 0.3393\n",
      "[  8.89%] Running Test Accuracy: 0.3242\n",
      "[ 10.00%] Running Test Accuracy: 0.3368\n",
      "[ 11.11%] Running Test Accuracy: 0.3312\n",
      "[ 12.22%] Running Test Accuracy: 0.3267\n",
      "[ 13.33%] Running Test Accuracy: 0.3229\n",
      "[ 14.44%] Running Test Accuracy: 0.3245\n",
      "[ 15.56%] Running Test Accuracy: 0.3281\n",
      "[ 16.67%] Running Test Accuracy: 0.3271\n",
      "[ 17.78%] Running Test Accuracy: 0.3242\n",
      "[ 18.89%] Running Test Accuracy: 0.3272\n",
      "[ 20.00%] Running Test Accuracy: 0.3316\n",
      "[ 21.11%] Running Test Accuracy: 0.3355\n",
      "[ 22.22%] Running Test Accuracy: 0.3422\n",
      "[ 23.33%] Running Test Accuracy: 0.3378\n",
      "[ 24.44%] Running Test Accuracy: 0.3338\n",
      "[ 25.56%] Running Test Accuracy: 0.3288\n",
      "[ 26.67%] Running Test Accuracy: 0.3320\n",
      "[ 27.78%] Running Test Accuracy: 0.3362\n",
      "[ 28.89%] Running Test Accuracy: 0.3365\n",
      "[ 30.00%] Running Test Accuracy: 0.3368\n",
      "[ 31.11%] Running Test Accuracy: 0.3371\n",
      "[ 32.22%] Running Test Accuracy: 0.3351\n",
      "[ 33.33%] Running Test Accuracy: 0.3333\n",
      "[ 34.44%] Running Test Accuracy: 0.3306\n",
      "[ 35.56%] Running Test Accuracy: 0.3369\n",
      "[ 36.67%] Running Test Accuracy: 0.3390\n",
      "[ 37.78%] Running Test Accuracy: 0.3401\n",
      "[ 38.89%] Running Test Accuracy: 0.3375\n",
      "[ 40.00%] Running Test Accuracy: 0.3385\n",
      "[ 41.11%] Running Test Accuracy: 0.3353\n",
      "[ 42.22%] Running Test Accuracy: 0.3363\n",
      "[ 43.33%] Running Test Accuracy: 0.3349\n",
      "[ 44.44%] Running Test Accuracy: 0.3367\n",
      "[ 45.56%] Running Test Accuracy: 0.3338\n",
      "[ 46.67%] Running Test Accuracy: 0.3341\n",
      "[ 47.78%] Running Test Accuracy: 0.3336\n",
      "[ 48.89%] Running Test Accuracy: 0.3331\n",
      "[ 50.00%] Running Test Accuracy: 0.3333\n",
      "[ 51.11%] Running Test Accuracy: 0.3336\n",
      "[ 52.22%] Running Test Accuracy: 0.3338\n",
      "[ 53.33%] Running Test Accuracy: 0.3320\n",
      "[ 54.44%] Running Test Accuracy: 0.3323\n",
      "[ 55.56%] Running Test Accuracy: 0.3331\n",
      "[ 56.67%] Running Test Accuracy: 0.3333\n",
      "[ 57.78%] Running Test Accuracy: 0.3329\n",
      "[ 58.89%] Running Test Accuracy: 0.3314\n",
      "[ 60.00%] Running Test Accuracy: 0.3316\n",
      "[ 61.11%] Running Test Accuracy: 0.3312\n",
      "[ 62.22%] Running Test Accuracy: 0.3331\n",
      "[ 63.33%] Running Test Accuracy: 0.3311\n",
      "[ 64.44%] Running Test Accuracy: 0.3319\n",
      "[ 65.56%] Running Test Accuracy: 0.3310\n",
      "[ 66.67%] Running Test Accuracy: 0.3307\n",
      "[ 67.78%] Running Test Accuracy: 0.3299\n",
      "[ 68.89%] Running Test Accuracy: 0.3301\n",
      "[ 70.00%] Running Test Accuracy: 0.3313\n",
      "[ 71.11%] Running Test Accuracy: 0.3286\n",
      "[ 72.22%] Running Test Accuracy: 0.3293\n",
      "[ 73.33%] Running Test Accuracy: 0.3310\n",
      "[ 74.44%] Running Test Accuracy: 0.3321\n",
      "[ 75.56%] Running Test Accuracy: 0.3332\n",
      "[ 76.67%] Running Test Accuracy: 0.3351\n",
      "[ 77.78%] Running Test Accuracy: 0.3344\n",
      "[ 78.89%] Running Test Accuracy: 0.3319\n",
      "[ 80.00%] Running Test Accuracy: 0.3329\n",
      "[ 81.11%] Running Test Accuracy: 0.3339\n",
      "[ 82.22%] Running Test Accuracy: 0.3336\n",
      "[ 83.33%] Running Test Accuracy: 0.3342\n",
      "[ 84.44%] Running Test Accuracy: 0.3339\n",
      "[ 85.56%] Running Test Accuracy: 0.3324\n",
      "[ 86.67%] Running Test Accuracy: 0.3321\n",
      "[ 87.78%] Running Test Accuracy: 0.3323\n",
      "[ 88.89%] Running Test Accuracy: 0.3320\n",
      "[ 90.00%] Running Test Accuracy: 0.3322\n",
      "[ 91.11%] Running Test Accuracy: 0.3346\n",
      "[ 92.22%] Running Test Accuracy: 0.3336\n",
      "[ 93.33%] Running Test Accuracy: 0.3330\n",
      "[ 94.44%] Running Test Accuracy: 0.3331\n",
      "[ 95.56%] Running Test Accuracy: 0.3314\n",
      "[ 96.67%] Running Test Accuracy: 0.3315\n",
      "[ 97.78%] Running Test Accuracy: 0.3320\n",
      "[ 98.89%] Running Test Accuracy: 0.3315\n",
      "[100.00%] Running Test Accuracy: 0.3323\n"
     ]
    }
   ],
   "source": [
    "path_pretrain = 'checkpoints/epoch3_batch900.pt'\n",
    "\n",
    "# Instantiate model + scorer\n",
    "if path_pretrain is not None:\n",
    "    model = ResponseScorer(base_model, freeze_base=False, device=device)\n",
    "    model.load_state_dict(torch.load(path_pretrain, map_location=device))\n",
    "    model.to(device)\n",
    "else:\n",
    "    model = ResponseScorer(base_model, freeze_base=False, device=device)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "num_batches = len(test_loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader, 1):\n",
    "        input_ids_a = batch[\"input_ids_a\"].to(device)\n",
    "        mask_a = batch[\"attention_mask_a\"].to(device)\n",
    "        input_ids_b = batch[\"input_ids_b\"].to(device)\n",
    "        mask_b = batch[\"attention_mask_b\"].to(device)\n",
    "\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        pct = 100 * i / num_batches\n",
    "        print(f\"[{pct:6.2f}%] Running Test Accuracy: {correct / total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02865bb6",
   "metadata": {},
   "source": [
    "### Kaggle Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "abd42a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>[\"I have three oranges today, I ate an orange ...</td>\n",
       "      <td>[\"You have two oranges today.\"]</td>\n",
       "      <td>[\"You still have three oranges. Eating an oran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>[\"You are a mediator in a heated political deb...</td>\n",
       "      <td>[\"Thank you for sharing the details of the sit...</td>\n",
       "      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>[\"How to initialize the classification head wh...</td>\n",
       "      <td>[\"When you want to initialize the classificati...</td>\n",
       "      <td>[\"To initialize the classification head when p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0   136060  [\"I have three oranges today, I ate an orange ...   \n",
       "1   211333  [\"You are a mediator in a heated political deb...   \n",
       "2  1233961  [\"How to initialize the classification head wh...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                    [\"You have two oranges today.\"]   \n",
       "1  [\"Thank you for sharing the details of the sit...   \n",
       "2  [\"When you want to initialize the classificati...   \n",
       "\n",
       "                                          response_b  \n",
       "0  [\"You still have three oranges. Eating an oran...  \n",
       "1  [\"Mr Reddy and Ms Blue both have valid points ...  \n",
       "2  [\"To initialize the classification head when p...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6ff8ca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction is tensor([1], device='xpu:0') for test set 0\n",
      "prediction is tensor([1], device='xpu:0') for test set 1\n",
      "prediction is tensor([0], device='xpu:0') for test set 2\n"
     ]
    }
   ],
   "source": [
    "def kaggle_to_datset(row, max_length = 512):\n",
    "\n",
    "    tokens_a = tokenizer(\n",
    "                row[\"response_a\"], max_length=max_length,\n",
    "                    padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "                )\n",
    "    tokens_b = tokenizer(\n",
    "        row[\"response_b\"], max_length=max_length,\n",
    "        padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return [tokens_a[\"input_ids\"].reshape(1,-1), \n",
    "            tokens_a[\"attention_mask\"].reshape(1,-1), \n",
    "            tokens_b[\"input_ids\"].reshape(1,-1), \n",
    "            tokens_b[\"attention_mask\"].reshape(1,-1),]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(df_test.shape[0]):\n",
    "        test_set = kaggle_to_datset(df_test.iloc[i])\n",
    "        logits = model(*test_set)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        print(f'prediction is {preds} for test set {i}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
