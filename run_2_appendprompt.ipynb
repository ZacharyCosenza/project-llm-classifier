{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916e0658",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c6e0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>[\"How can I create a test set for a very rare ...</td>\n",
       "      <td>[\"Creating a test set for a very rare category...</td>\n",
       "      <td>[\"When building a classifier for a very rare c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n",
       "      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n",
       "      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "2  [\"explain function calling. how would you call...   \n",
       "3  [\"How can I create a test set for a very rare ...   \n",
       "4  [\"What is the best way to travel from Tel-Aviv...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "2  [\"Function calling is the process of invoking ...   \n",
       "3  [\"Creating a test set for a very rare category...   \n",
       "4  [\"The best way to travel from Tel Aviv to Jeru...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "2  [\"Function calling is the process of invoking ...               0   \n",
       "3  [\"When building a classifier for a very rare c...               1   \n",
       "4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  \n",
       "2               0           1  \n",
       "3               0           0  \n",
       "4               1           0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = 'data/train.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c345cb",
   "metadata": {},
   "source": [
    "### Download/Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a75c09c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacco\\OneDrive\\Documents\\Personal\\Code\\venvs\\xpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: xpu\n",
      "Loaded saved DistilBERT weights.\n",
      "Model type: DistilBertModel\n",
      "Tokenizer vocab size: 30522\n",
      "Hidden size: 768, Layers: 6, Heads: 12\n",
      "Inference QC: last_hidden_state shape torch.Size([1, 9, 768])\n",
      "Training QC: backward pass successful (dummy loss=-0.0098)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
    "import os\n",
    "\n",
    "weights = 'pretrained'\n",
    "model_dir = \"weights/distilbert\"\n",
    "\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Option A: Download and save pretrained weights\n",
    "def download_and_save():\n",
    "    model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model.save_pretrained(model_dir)\n",
    "    print(\"Downloaded and saved pretrained DistilBERT weights.\")\n",
    "    return model\n",
    "\n",
    "# Option B: Load saved weights (if available)\n",
    "def load_saved():\n",
    "    model = DistilBertModel.from_pretrained(model_dir)\n",
    "    print(\"Loaded saved DistilBERT weights.\")\n",
    "    return model\n",
    "\n",
    "# Option C: Randomly initialize weights\n",
    "def init_random():\n",
    "    config = DistilBertConfig()\n",
    "    model = DistilBertModel(config)\n",
    "    print(\"Initialized DistilBERT with random weights.\")\n",
    "    return model\n",
    "\n",
    "# Load weights\n",
    "if os.path.exists(os.path.join(model_dir, \"model.safetensors\")):\n",
    "    base_model = load_saved()\n",
    "else:\n",
    "    base_model = download_and_save() if weights == 'pretrained' else init_random()\n",
    "\n",
    "# --- Model and Tokenizer ---\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(f\"Model type: {type(base_model).__name__}\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Hidden size: {base_model.config.dim}, Layers: {base_model.config.n_layers}, Heads: {base_model.config.n_heads}\")\n",
    "\n",
    "# --- Inference QC ---\n",
    "sample_text = \"I love using Hugging Face Transformers!\"\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\").to(device)\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = base_model(**inputs)\n",
    "\n",
    "print(f\"Inference QC: last_hidden_state shape {outputs.last_hidden_state.shape}\")\n",
    "\n",
    "# --- Training QC ---\n",
    "base_model.train()\n",
    "inputs = tokenizer(list(df[\"prompt\"][:2]), return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "optimizer = torch.optim.AdamW(base_model.parameters(), lr=1e-5)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "outputs = base_model(**inputs)\n",
    "loss = outputs.last_hidden_state.mean()  # dummy scalar loss just for gradient test\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(f\"Training QC: backward pass successful (dummy loss={loss.item():.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afef08f",
   "metadata": {},
   "source": [
    "### Dataset/Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79b60b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ResponseDataset(Dataset):\n",
    "    def __init__(self, df, max_length=512):\n",
    "        self.df = df\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def outcome_to_class(self, row):\n",
    "        if row[\"winner_model_a\"] == 1:\n",
    "            return 2   # A wins\n",
    "        if row[\"winner_model_b\"] == 1:\n",
    "            return 0   # B wins\n",
    "        return 1       # tie\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # concatenate prompt and response\n",
    "        full_a = row[\"prompt\"] + row[\"response_a\"]\n",
    "        full_b = row[\"prompt\"] + row[\"response_b\"]\n",
    "\n",
    "        tokens_a = tokenizer(\n",
    "            full_a, max_length=self.max_length,\n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        tokens_b = tokenizer(\n",
    "            full_b, max_length=self.max_length,\n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        label = torch.tensor(self.outcome_to_class(row), dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids_a\": tokens_a[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_a\": tokens_a[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_b\": tokens_b[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_b\": tokens_b[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": label,\n",
    "        }\n",
    "\n",
    "df_train, df_temp = train_test_split(df, test_size=0.3, random_state=42, shuffle=True)\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ResponseDataset(df_train)\n",
    "val_dataset = ResponseDataset(df_val)\n",
    "test_dataset = ResponseDataset(df_test)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573564ad",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d962bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ResponseScorer(nn.Module):\n",
    "    def __init__(self, base_model, freeze_base=False, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        hidden_dim = self.base_model.config.dim\n",
    "\n",
    "        # classifier head â†’ outputs logits for [B wins, Tie, A wins]\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, 3)   # 3-class output\n",
    "        )\n",
    "\n",
    "        if freeze_base:\n",
    "            for p in self.base_model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        out = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return out.last_hidden_state[:, 0, :]  # CLS\n",
    "\n",
    "    def forward(self, input_ids_a, mask_a, input_ids_b, mask_b):\n",
    "        # embeddings from base model\n",
    "        h_a = self.encode(input_ids_a.to(self.device), mask_a.to(self.device))\n",
    "        h_b = self.encode(input_ids_b.to(self.device), mask_b.to(self.device))\n",
    "\n",
    "        # concatenate the two embeddings\n",
    "        h = torch.cat([h_a, h_b], dim=-1)\n",
    "\n",
    "        # 3-class logits\n",
    "        logits = self.head(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c9ab04",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a79a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4.0% done | Avg Loss: 1.0957\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch100.pt\n",
      "Epoch 1: 8.0% done | Avg Loss: 1.0928\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch200.pt\n",
      "Epoch 1: 11.9% done | Avg Loss: 1.0912\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch300.pt\n",
      "Epoch 1: 15.9% done | Avg Loss: 1.0886\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch400.pt\n",
      "Epoch 1: 19.9% done | Avg Loss: 1.0851\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch500.pt\n",
      "Epoch 1: 23.9% done | Avg Loss: 1.0835\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch600.pt\n",
      "Epoch 1: 27.8% done | Avg Loss: 1.0801\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch700.pt\n",
      "Epoch 1: 31.8% done | Avg Loss: 1.0781\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch800.pt\n",
      "Epoch 1: 35.8% done | Avg Loss: 1.0769\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch900.pt\n",
      "Epoch 1: 39.8% done | Avg Loss: 1.0747\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch1000.pt\n",
      "Epoch 1: 43.7% done | Avg Loss: 1.0738\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch1100.pt\n",
      "Epoch 1: 47.7% done | Avg Loss: 1.0729\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch1200.pt\n",
      "Epoch 1: 51.7% done | Avg Loss: 1.0722\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch1300.pt\n",
      "Epoch 1: 55.7% done | Avg Loss: 1.0713\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch1400.pt\n",
      "Epoch 1: 59.6% done | Avg Loss: 1.0704\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch1500.pt\n",
      "Epoch 1: 63.6% done | Avg Loss: 1.0698\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch1600.pt\n",
      "Epoch 1: 67.6% done | Avg Loss: 1.0686\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch1700.pt\n",
      "Epoch 1: 71.6% done | Avg Loss: 1.0678\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch1800.pt\n",
      "Epoch 1: 75.5% done | Avg Loss: 1.0670\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch1900.pt\n",
      "Epoch 1: 79.5% done | Avg Loss: 1.0657\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch2000.pt\n",
      "Epoch 1: 83.5% done | Avg Loss: 1.0652\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch2100.pt\n",
      "Epoch 1: 87.5% done | Avg Loss: 1.0645\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch2200.pt\n",
      "Epoch 1: 91.5% done | Avg Loss: 1.0638\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch2300.pt\n",
      "Epoch 1: 95.4% done | Avg Loss: 1.0626\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch2400.pt\n",
      "Epoch 1: 99.4% done | Avg Loss: 1.0620\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch1_batch2500.pt\n",
      "Epoch 1: 100.0% done | Avg Loss: 1.0619\n",
      "Epoch 2: 4.0% done | Avg Loss: 1.0379\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch2_batch100.pt\n",
      "Epoch 2: 8.0% done | Avg Loss: 1.0365\n",
      "Saved checkpoint to weights/checkpoints/run_2/epoch2_batch200.pt\n",
      "Epoch 2: 9.4% done | Avg Loss: 1.0361"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Native API failed. Native API returns: 20 (UR_RESULT_ERROR_DEVICE_LOST)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m total_batches = \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     input_ids_a = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids_a\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     mask_a = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask_a\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m     28\u001b[39m     input_ids_b = batch[\u001b[33m\"\u001b[39m\u001b[33minput_ids_b\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n",
      "\u001b[31mRuntimeError\u001b[39m: Native API failed. Native API returns: 20 (UR_RESULT_ERROR_DEVICE_LOST)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "mode = 'train'\n",
    "path_save = \"weights/checkpoints/run_2\"\n",
    "\n",
    "model = ResponseScorer(base_model, freeze_base=False, device=device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "if mode == 'train':\n",
    "\n",
    "    num_epochs = 3\n",
    "    val_losses, val_accuracies = [], []\n",
    "\n",
    "    # Create directory for checkpoints\n",
    "    if not os.path.isdir(path_save):\n",
    "        os.makedirs(path_save, exist_ok=True)\n",
    "    save_every = 100  # batches\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(train_loader)\n",
    "\n",
    "        for i, batch in enumerate(train_loader, 1):\n",
    "            input_ids_a = batch[\"input_ids_a\"].to(device)\n",
    "            mask_a = batch[\"attention_mask_a\"].to(device)\n",
    "            input_ids_b = batch[\"input_ids_b\"].to(device)\n",
    "            mask_b = batch[\"attention_mask_b\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)        # <-- single class ID 0/1/2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # logits: [batch, 3]\n",
    "            logits = model(input_ids_a, mask_a, input_ids_b, mask_b)\n",
    "\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # ---- progress indicator ----\n",
    "            pct_done = 100 * i / total_batches\n",
    "            print(\n",
    "                f\"\\rEpoch {epoch}: {pct_done:.1f}% done | Avg Loss: {running_loss / i:.4f}\",\n",
    "                end=\"\"\n",
    "            )\n",
    "\n",
    "            # ---- periodic save ----\n",
    "            if i % save_every == 0:\n",
    "                ckpt_path = f\"{path_save}/epoch{epoch}_batch{i}.pt\"\n",
    "                torch.save(model.state_dict(), ckpt_path)\n",
    "                print(f\"\\nSaved checkpoint to {ckpt_path}\")\n",
    "\n",
    "        print()  # newline after epoch progress\n",
    "\n",
    "        # Save final epoch weights\n",
    "        torch.save(model.state_dict(), f\"{path_save}/epoch{epoch}_final.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15be954",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee60d251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\zacco\\AppData\\Local\\Temp\\ipykernel_22836\\1247110618.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  path_pretrain = 'weights\\checkpoints\\run_2\\epoch2_batch200.pt'\n",
      "C:\\Users\\zacco\\AppData\\Local\\Temp\\ipykernel_22836\\1247110618.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  path_pretrain = 'weights\\checkpoints\\run_2\\epoch2_batch200.pt'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Native API failed. Native API returns: 39 (UR_RESULT_ERROR_OUT_OF_DEVICE_MEMORY)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Instantiate model + scorer\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path_pretrain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     model = \u001b[43mResponseScorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze_base\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     model.load_state_dict(torch.load(path_pretrain, map_location=device))\n\u001b[32m      7\u001b[39m     model.to(device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mResponseScorer.__init__\u001b[39m\u001b[34m(self, base_model, freeze_base, device)\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.base_model.parameters():\n\u001b[32m     22\u001b[39m         p.requires_grad = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mself\u001b[39m.device = device\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zacco\\OneDrive\\Documents\\Personal\\Code\\venvs\\xpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zacco\\OneDrive\\Documents\\Personal\\Code\\venvs\\xpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zacco\\OneDrive\\Documents\\Personal\\Code\\venvs\\xpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zacco\\OneDrive\\Documents\\Personal\\Code\\venvs\\xpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zacco\\OneDrive\\Documents\\Personal\\Code\\venvs\\xpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: Native API failed. Native API returns: 39 (UR_RESULT_ERROR_OUT_OF_DEVICE_MEMORY)"
     ]
    }
   ],
   "source": [
    "path_pretrain = 'weights\\checkpoints\\run_2\\epoch2_batch200.pt'\n",
    "\n",
    "# Instantiate model + scorer\n",
    "if path_pretrain is not None:\n",
    "    model = ResponseScorer(base_model, freeze_base=False, device=device)\n",
    "    model.load_state_dict(torch.load(path_pretrain, map_location=device))\n",
    "    model.to(device)\n",
    "else:\n",
    "    model = ResponseScorer(base_model, freeze_base=False, device=device)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "num_batches = len(test_loader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader, 1):\n",
    "        input_ids_a = batch[\"input_ids_a\"].to(device)\n",
    "        mask_a = batch[\"attention_mask_a\"].to(device)\n",
    "        input_ids_b = batch[\"input_ids_b\"].to(device)\n",
    "        mask_b = batch[\"attention_mask_b\"].to(device)\n",
    "\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        pct = 100 * i / num_batches\n",
    "        print(f\"[{pct:6.2f}%] Running Test Accuracy: {correct / total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02865bb6",
   "metadata": {},
   "source": [
    "### Kaggle Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd42a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>[\"I have three oranges today, I ate an orange ...</td>\n",
       "      <td>[\"You have two oranges today.\"]</td>\n",
       "      <td>[\"You still have three oranges. Eating an oran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>[\"You are a mediator in a heated political deb...</td>\n",
       "      <td>[\"Thank you for sharing the details of the sit...</td>\n",
       "      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>[\"How to initialize the classification head wh...</td>\n",
       "      <td>[\"When you want to initialize the classificati...</td>\n",
       "      <td>[\"To initialize the classification head when p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0   136060  [\"I have three oranges today, I ate an orange ...   \n",
       "1   211333  [\"You are a mediator in a heated political deb...   \n",
       "2  1233961  [\"How to initialize the classification head wh...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                    [\"You have two oranges today.\"]   \n",
       "1  [\"Thank you for sharing the details of the sit...   \n",
       "2  [\"When you want to initialize the classificati...   \n",
       "\n",
       "                                          response_b  \n",
       "0  [\"You still have three oranges. Eating an oran...  \n",
       "1  [\"Mr Reddy and Ms Blue both have valid points ...  \n",
       "2  [\"To initialize the classification head when p...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test = pd.read_csv('data/test.csv')\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8ca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction is tensor([1], device='xpu:0') for test set 0\n",
      "prediction is tensor([1], device='xpu:0') for test set 1\n",
      "prediction is tensor([0], device='xpu:0') for test set 2\n"
     ]
    }
   ],
   "source": [
    "# def kaggle_to_datset(row, max_length = 512):\n",
    "\n",
    "#     tokens_a = tokenizer(\n",
    "#                 row[\"response_a\"], max_length=max_length,\n",
    "#                     padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "#                 )\n",
    "#     tokens_b = tokenizer(\n",
    "#         row[\"response_b\"], max_length=max_length,\n",
    "#         padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "#     )\n",
    "\n",
    "#     return [tokens_a[\"input_ids\"].reshape(1,-1), \n",
    "#             tokens_a[\"attention_mask\"].reshape(1,-1), \n",
    "#             tokens_b[\"input_ids\"].reshape(1,-1), \n",
    "#             tokens_b[\"attention_mask\"].reshape(1,-1),]\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i in range(df_test.shape[0]):\n",
    "#         test_set = kaggle_to_datset(df_test.iloc[i])\n",
    "#         logits = model(*test_set)\n",
    "#         preds = logits.argmax(dim=-1)\n",
    "#         print(f'prediction is {preds} for test set {i}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
